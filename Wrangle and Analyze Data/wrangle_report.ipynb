{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Introduction](#intro)\n",
    "\n",
    "- [Gather](#gather)\n",
    "\n",
    "- [Assess](#assess)\n",
    "\n",
    "- [Clean](#clean)\n",
    "\n",
    "- [Storing and Acting on Wrangled Data](#store)\n",
    "\n",
    "- [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "This paper here is a description of the process of the data wrangling on the wrangle_act paper.\n",
    "\n",
    "But befor we start, let's recape what we learn before\n",
    "\n",
    "As we saw in lesson 1 : Introduction to data wrangling. There are 3 main parts of this process\n",
    "\n",
    "- Gathering data\n",
    "    - Depending on the source of your data\n",
    "    - what format it's in\n",
    "    - obtaining data (downloading a file from the internet, scraping a web page, querying an API, etc.)\n",
    "    - importing that data into your programming environment (e.g., Jupyter Notebook).\n",
    "    \n",
    "- Assessing data\n",
    "    - Quality\n",
    "        - issues with content\n",
    "    - Tidiness\n",
    "        - Each variable forms a column.\n",
    "        - Each observation forms a row.\n",
    "        - Each type of observational unit forms a table.\n",
    "\n",
    "There are two Types of assessment:\n",
    "    - Visual assessment:\n",
    "        - scrolling through the data in your preferred software application (Google Sheets, Excel, a text editor, etc.).\n",
    "    \n",
    "    - Programmatic assessment:\n",
    "        - using code to view specific portions and summaries of the data (pandas' head, tail, and info methods, for example).\n",
    "    \n",
    "\n",
    "- Cleaning data\n",
    "    - Define\n",
    "    - Code\n",
    "    - Test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather\n",
    "\n",
    "In this project, there were three different types of data sets that we needed. Different format. Different approach of collection.\n",
    "\n",
    "First. We store the CSV file twitter_archive_enhanced.csv manually.\n",
    "\n",
    "After that, We store the TSV file image_predictions.tsv programmatically using python Requests library.\n",
    "\n",
    "And finally, we store the JSON file tweet_json.txt using Twitter API key and using Python's Tweepy library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess\n",
    "\n",
    "After gathering the data, it was time to take a look and try to spot anything that doesn't seem to be right.\n",
    "\n",
    "In this step, we can do this either visually or programmatically. However doing it visually will be very hard and it will take a lot of time.\n",
    "\n",
    "Here is what we found:\n",
    "\n",
    "- Quality\n",
    "    - twitter_archive_enhanced.csv \n",
    "        - The following attributes should be (int) or (string) instead of float\n",
    "        [in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id].\n",
    "        String is better since we are not going to do any calculations.\n",
    "    \n",
    "        - The following attributes should be (datetime) instead of String\n",
    "        [timestamp, retweeted_status_timestamp]\n",
    "    \n",
    "        - The following attributes have some invalid values\n",
    "        [name, rating_denominator, rating_numerator]\n",
    "        \n",
    "        - The following attributes are not needed since we only care about original tweet\n",
    "        [in_reply_to_user_id, in_reply_to_status_id, retweeted_status_id, retweeted_status_user_id,retweeted_status_timestamp]\n",
    "    \n",
    "    - image_predictions.tsv\n",
    "        - This dataset is missing some values.\n",
    "        \n",
    "        - Some jpg_url are used for more than one tweet.\n",
    "    \n",
    "    - tweet_json.txt\n",
    "        \n",
    "        - The following attributes should be (datetime) instead of String\n",
    "        [created_at]\n",
    "        \n",
    "        - This tweet [666020888022790149] is duplicated 20 times.\n",
    "        \n",
    "        - The attribute [created_at] is the same as The attribute [timestamp]. So we don't need both\n",
    "\n",
    "\n",
    "- Tidiness\n",
    "    - twitter_archive_enhanced\n",
    "        - Not all attributes are important in this data frame.\n",
    "        - The following attributes are better represented as data categories in a single column\n",
    "        [Doggo, floofer, pupper, puppo]\n",
    "        - source looks messy. There is a better way to do it\n",
    "    \n",
    "    - image_predictions.tsv\n",
    "        - Not all attributes are important in this data frame.\n",
    "    \n",
    "    - tweet_json.txt\n",
    "        - We need one table that is a subset of these 3 tables\n",
    "        - Number of rows is not the same in these 3 tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Clean\n",
    "\n",
    "This is the third and last step of the data wrangling process.\n",
    "\n",
    "This part could be done either manually or programmatically. Doing it manually might not be the best idea.\n",
    "\n",
    "Here we fixed our findings in the previous step. And after that, there is always reassessing and iterating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='store'></a>\n",
    "## Storing and Acting on Wrangled Data\n",
    "\n",
    "In this step, we store the data set that contains all the three data sets as a CVS file and as a database file (DB).\n",
    "\n",
    "And after that, we did some work on the new clean data set. Trying to find some relations and some new findings. This part you can see in a separate paper called act_report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "Not all the data that you will come across are clean, in fact, almost all the data sets that you will come across are not clean. That's why data wrangling is such an important skill to have.\n",
    "\n",
    "This project was very helpful to me. It did help me to understand and to pay attention to every data set I see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
